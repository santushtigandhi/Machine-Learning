{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expensive-morocco",
   "metadata": {},
   "source": [
    "<b><center>Ridge and Lasso Regression</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-following",
   "metadata": {},
   "source": [
    "Before going into the algorithm let's understand what is Overfitting and Underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-speaker",
   "metadata": {},
   "source": [
    "So as to understand Overfitting and Underfitting more clearly, we need to learn the terms - Bias and Variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-strain",
   "metadata": {},
   "source": [
    "In simple words, Bias in the training data error and Variance is the test data error.\n",
    "- Low Bias is when we have less training data error, this means that the the distance between the actual points and regression line is less.\n",
    "- High Bias is when we have high training data error, this means that the the distance between the actual points and regression line is high.\n",
    "- Low Variance is when we have less testing data error, this means that the the distance between the predicted points and regression line is less.\n",
    "- High Variance is when we have high testing data error, this means that the the distance between the predicted points and regression line is high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-variation",
   "metadata": {},
   "source": [
    "Now let's see the case of <b>Underfitting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-moisture",
   "metadata": {},
   "source": [
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-and-underfitting2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-merchant",
   "metadata": {},
   "source": [
    "Here we see we have high bias(since the distance between the actual points and regression line is high) and high variance (since the distance between the predicted points and regression line would also be high) <br>\n",
    "This is the case of Underfitting. <br>\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. It occurs when a model is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-guitar",
   "metadata": {},
   "source": [
    "Let's see the case of <b>Overfitting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-longitude",
   "metadata": {},
   "source": [
    "<img src=\"https://lh5.googleusercontent.com/KyrpL6RKZ-M1_EANSCI_FulanBBVXdHAho4RSD5d75VWWN7sMzYUMFHuaDW5oWRRZ9lHQZqEYnYhQ0oiwOq0skNqbmhBoViW6Ij187Cm8tmRII2W-FIeFAESnPvHBC7B369WP_fJaUf7UPvQ\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-desperate",
   "metadata": {},
   "source": [
    "Here we see we have low bias(since the distance between the actual points and regression line is low) and high variance (since the distance between the predicted points and regression line would be high) <br>\n",
    "This is the case of Overfitting. <br>\n",
    "Overfitting is a concept in data science, which occurs when a statistical model fits exactly against its training data. When this happens, the algorithm unfortunately cannot perform accurately against unseen data, defeating its purpose. However, when the model trains for too long on sample data or when the model is too complex, it can start to learn the “noise,” or irrelevant information, within the dataset. When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data.  If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-consumption",
   "metadata": {},
   "source": [
    "<b>Note :</b> We will have a <b>proper fit</b> when we have low bias(the distance between the actual points and regression line is low) and low variance(the distance between the predicted points and regression line is low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-motion",
   "metadata": {},
   "source": [
    "<img src=\"https://1.cms.s81c.com/sites/default/files/2021-03-03/model-over-fitting.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-exception",
   "metadata": {},
   "source": [
    "<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1543418451/bias_vs_variance_swxhxx.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-princeton",
   "metadata": {},
   "source": [
    "Now, let's see the graph of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-sterling",
   "metadata": {},
   "source": [
    "<img src=\"https://www.goeduhub.com/?qa=blob&qa_blobid=17138642511027821&ezimgfmt=rs:350x220/rscb1/ng:webp/ngcb1\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-castle",
   "metadata": {},
   "source": [
    "Here, we see the point which is called Optimal model, this is the point where we have low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-baseball",
   "metadata": {},
   "source": [
    "Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-sheet",
   "metadata": {},
   "source": [
    "<b> Ridge Regression </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-terminology",
   "metadata": {},
   "source": [
    "A regression model that uses L2 regularization technique is called Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-greensboro",
   "metadata": {},
   "source": [
    "In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-protein",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/486/1*jgWOhDiGjVp-NCSPa5abmg.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-bottle",
   "metadata": {},
   "source": [
    "This is the cost function for Ridge Regression. <br>\n",
    "Here the highlighted part represents L2 regularization element. Here, if lambda is zero then you can imagine we get back OLS (I'll explain this term later in  this repository). However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-springer",
   "metadata": {},
   "source": [
    "This is equivalent to saying minimizing the cost function under the condition as below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-glenn",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*sC4KLMHU0j_1gR3VmlgGtg.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-shepherd",
   "metadata": {},
   "source": [
    "So ridge regression puts constraint on the coefficients (w). The penalty term (lambda) regularizes the coefficients such that if the coefficients take large values the optimization function is penalized. So, ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-accessory",
   "metadata": {},
   "source": [
    "<b>Lasso Regression</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-phone",
   "metadata": {},
   "source": [
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-meter",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/240/1*4MlW1d3xszVAGuXiJ1U6Fg.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-chorus",
   "metadata": {},
   "source": [
    "This is the cost function for Lasso Regression. <br>\n",
    "Here the highlighted part represents L1 regularization element. Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-belly",
   "metadata": {},
   "source": [
    "L1 regularization forces the weights of uninformative features to be zero by substracting a small amount from the weight at each iteration and thus making the weight zero, eventually. <br>\n",
    "L1 regularization penalizes |weight|."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-tactics",
   "metadata": {},
   "source": [
    "However, L2 regularization forces weights toward zero but it does not make them exactly zero. L2 regularization acts like a force that removes a small percentage of weights at each iteration. Therefore, weights will never be equal to zero. <br>\n",
    "L2 regularization penalizes (weight)²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-consideration",
   "metadata": {},
   "source": [
    "<b>Note:</b> Choosing an optimal value for lambda is important. If lambda is too high, the model becomes too simple and tends to underfit. On the other hand, if lambda is too low, the effect of regulatization becomes negligible and the model is likely to overfit. If lambda is set to zero, then regularization will be completely removed (high risk of overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-pattern",
   "metadata": {},
   "source": [
    "<b>Now, why does L1 regularization results in Sparse solution?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-committee",
   "metadata": {},
   "source": [
    "<img src=\"https://qphs.fs.quoracdn.net/main-qimg-fc2b7b86094539a7d72269af62729f75-lq\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-blink",
   "metadata": {},
   "source": [
    "L1 & L2 regularization add constraints to the optimization problem. The curve H0 is the hypothesis. The solution to this system is the set of points where the H0 meets the constraints. <br> \n",
    "\n",
    "Now, in the case of L2 regularization, in most cases, the the hypothesis is tangential hence, the point of intersection has both x1 and x2 components. On the other hand, in L1, due to the nature of the curve, the viable solutions are limited to the corners, which are on one axis only - in the above case x1. Value of x2 = 0. This means that the solution has eliminated the role of x2 leading to sparsity. Extend this to higher dimensions and you can see why L1 regularization leads to solutions to the optimization problem where many of the variables have value 0. <br>\n",
    "\n",
    "In other words, L1 regularization leads to sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-style",
   "metadata": {},
   "source": [
    "Lets's understand the last topic of this repository - <b>OLS (Ordinary Least Squares)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-flesh",
   "metadata": {},
   "source": [
    "OLS or Ordinary Least Squares is a method in Linear Regression for estimating the unknown parameters by creating a model which will minimize the sum of the squared errors between the observed data and the predicted one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-glass",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1256/1*uBnjPy5o59FfkkMEJL0Nqw.jpeg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-wells",
   "metadata": {},
   "source": [
    "For example, consider the above picture. We have a scatter plot where each dot represents the data points. Using linear regression model, a straight line is fitted. The objective here is to minimize the error between the data points (observed) and the points on the line (Predicted). Hence, keeping this in mind, we will calculate the sum of the vertical distances (shown as squares). The smaller the distance, the better model fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-shannon",
   "metadata": {},
   "source": [
    "Simple Linear Regression is a statistical model, widely used in ML regression tasks, based on the idea that the relationship between two variables can be explained by the following formula: <br>\n",
    "&emsp;&emsp;    yi = α + βxi + εi\n",
    "    \n",
    "where εi is the error term, and α, β are the true (but unobserved) parameters of the regression. <br>\n",
    "We have all studied the simple equation of line:<br>\n",
    "&emsp;&emsp;    y = mx + c <br>\n",
    "    In the above equation I have replaced m with β and c with α."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-shepherd",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*2VBMHbTdBI4cNozHhpTa0Q.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-genetics",
   "metadata": {},
   "source": [
    "So, in order to find the parameters α and β, we carry a procedure where we minimise the error component. <br>\n",
    "This procedure is called Ordinary Least Squared error — OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-section",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/443/1*7rSwjrW1pDS0V-Ag-zNL-Q.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-issue",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/627/1*dqwE7Yr-AUkT02_c5W6h2A.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-saturn",
   "metadata": {},
   "source": [
    "We can set our optimization problem as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-ethics",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/325/1*fcST4EnwYMo8vycM2kR9ew.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-mobility",
   "metadata": {},
   "source": [
    "By carrying out this derivative, we find the values of α and β as follows: <br>\n",
    "   β = Σ(xi - x̄)(yi - ȳ) / Σ(xi - x̄)^2  <br>\n",
    "   α = ȳ - m * x̄\n",
    "   - where, x = independent variables\n",
    "   - x̄ = average of independent variables\n",
    "   - y = dependent variables\n",
    "   - ȳ = average of dependent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-count",
   "metadata": {},
   "source": [
    "And that is how we find our linear regression best line fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
